{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods: Tree Bagging; Random Forests; Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked a bit in passing about a few ensemble methods when we talked about trees etc. Let's take some time to use them! We'll go over both the sklearn implementations, and try implementing both ourselves. In the 'do it yourself' part, I'll give you a single iteration, it is your job to put it together ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll instead use the famous *boston housing data* to try out ensemble methods. We are going to make the output binary, so that we can focus on classification. We'll do regression later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the boston housing data\n",
    "boston = load_boston()\n",
    "X, Y = boston.data, boston.target\n",
    "\n",
    "# binarize the output so it is now a classification task\n",
    "Y = 1 * (Y > np.median(Y))\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:350], Y[:350]\n",
    "test_data, test_labels = X[350:], Y[350:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following questions, you might find this function useful to print out the tree. If you want to try a graphical way, look into this function:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "\n",
    "The below function prints out a 'pseudocode' version of the tree, in terms of if-else statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_code(tree, feature_names):\n",
    "        left      = tree.tree_.children_left\n",
    "        right     = tree.tree_.children_right\n",
    "        threshold = tree.tree_.threshold\n",
    "        features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "        value = tree.tree_.value\n",
    "\n",
    "        def recurse(left, right, threshold, features, node):\n",
    "                if (threshold[node] != -2):\n",
    "                        print \"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\"\n",
    "                        if left[node] != -1:\n",
    "                                recurse (left, right, threshold, features,left[node])\n",
    "                        print \"} else {\"\n",
    "                        if right[node] != -1:\n",
    "                                recurse (left, right, threshold, features,right[node])\n",
    "                        print \"}\"\n",
    "                else:\n",
    "                        print \"return \" + str(value[node])\n",
    "\n",
    "        recurse(left, right, threshold, features, 0)\n",
    "\n",
    "# example call:\n",
    "# get_code(dt, boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what sklearn has in terms of ensemble methods. There are two interesting ones we can use right now, adaboost and random forests. We'll start by using the sklearn ones, then try implementing random forests ourselves!\n",
    "\n",
    "Be sure to reference the documentation at:  http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Let's start with just executing some sklearn functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (a decision tree): 0.878205128205\n",
      "Accuracy (a random forest): 0.910256410256\n",
      "Accuracy (adaboost with decision trees): 0.910256410256\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "dt.fit(train_data, train_labels)\n",
    "\n",
    "print 'Accuracy (a decision tree):', dt.score(test_data, test_labels)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(train_data, train_labels)\n",
    "\n",
    "print 'Accuracy (a random forest):', rfc.score(test_data, test_labels)\n",
    "\n",
    "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "abc.fit(train_data, train_labels)\n",
    "print 'Accuracy (adaboost with decision trees):', abc.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like ensemble methods do well, both do better than a single tree. Before moving on, try playing arond with some of the parameters, such as:\n",
    "\n",
    "n_estimators in RandomForestClassifier\n",
    "\n",
    "\n",
    "n_estimators and learning_rate AdaBoostClassifier\n",
    "\n",
    "Why do the methods behave as they when you tweak the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are combinations of many decision trees. Let's start with a slightly simplified version: **tree bagging**. Here is a simple algorithm for tree bagging:\n",
    "\n",
    "1. Set B (number of trees to make)\n",
    "2. Repeat B times:\n",
    "  1. Draw N random samples from training data, with replacement, where N is the number of training data points\n",
    "  2. Fit a decision tree to this re-sampled data\n",
    "  3. Store the predictions from this decision tree on the test data\n",
    "3. As the final predictions on the test data, use the majority vote classification for the predictions above\n",
    "\n",
    "Below, I've given you an implementation of a single iteration of the main loop above. Complete the algorthim by (1) adding the repeated B resampling and fitting (2) implementing step 3 above, the final predictions from tree bagging.\n",
    "\n",
    "Once you've done that, does bagging do better than a single tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# the following can be replaced in a single line with:\n",
    "# np.random.choice(range(n), size=n, replace=True)\n",
    "# but I've given the explicit code so you can learn a bit :-)\n",
    "def bs_sample_index(n):\n",
    "    bootstrap_sample_index = np.random.rand(n)\n",
    "    bootstrap_sample_index = np.floor(bootstrap_sample_index * n)\n",
    "    bootstrap_sample_index = bootstrap_sample_index.astype(\"int\")\n",
    "    \n",
    "    return bootstrap_sample_index\n",
    "\n",
    "# a single iteration of tree bagging\n",
    "\n",
    "bootstrap_sample_index = bs_sample_index(train_data.shape[0])\n",
    "\n",
    "bs_data = train_data[bootstrap_sample_index, :]\n",
    "bs_labels = train_labels[bootstrap_sample_index]\n",
    "    \n",
    "# without max_feature restriction this is 'tree bagging'\n",
    "bs_tree = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\")\n",
    "bs_tree.fit(bs_data, bs_labels)\n",
    "\n",
    "bs_tree_preds = bs_tree.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to do **random forests**. Random forests add the twist of subsampling features at each node. Typically, we take p' = sqrt(p) features. DecisionTreeClassifier implements with through the *max_features*, check out the documentation. A simple change to your above code should give you random forests.\n",
    "\n",
    "Does random forests do better than tree bagging?\n",
    "\n",
    "Note: you can also use trees, tree bagging, and random forests for regression! Now, the original data is a regression problem so just reload the data, and to do all of these ideas using trees, you need only use DecisionTreeRegressor instead of DecisionTreeClassifier; see:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "As a bonus, try implementing trees, tree bagging, and random forests for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost is another popular ensemble method. It operates roughly in the following way:\n",
    "\n",
    "1. Initialize a set of weights over the data by weighting each datapoint equally. Set a number of learning rounds B\n",
    "2. Do the following B times:\n",
    "  1. *Train a simple classifier on the data*, using the current set of weights.\n",
    "  2. *Compute the overall error rate on the **training data** *, weighted by the current set of weights, of the classifier, **save this error rate**.\n",
    "  3. *Update the distribution of weights*, giving more importance to datapoints we misclassified\n",
    "3. Combine the classifiers of all the loop iterations together, weighted by a function (denoted error_rate_alpha below) of their error rates.\n",
    "\n",
    "Below, a single iteration of adaboost is implemented. \n",
    "\n",
    "Before extending it, check out how we update the weights. How does this emphasize errors made in the previous round?\n",
    "\n",
    "Now, extend it so it actually does multiple learning rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert labels into +/- 1; this is just how adaboost likes it :)\n",
    "train_labels_pm = train_labels * 2 - 1\n",
    "test_labels_pm = test_labels * 2 - 1\n",
    "\n",
    "# initialize with equal weights on each data point\n",
    "data_weights = np.ones(train_data.shape[0]).astype(\"float\") / float(train_data.shape[0])\n",
    "\n",
    "# here begins a single learning round\n",
    "bdtc = DecisionTreeClassifier(max_depth=1)\n",
    "bdtc.fit(train_data, train_labels_pm, sample_weight=data_weights)\n",
    "\n",
    "# you'll need to save bdtc_predictions each iteration of the loop to do final training set predictions\n",
    "bdtc_predictions = bdtc.predict(train_data)\n",
    "\n",
    "# you'll need to save bdtc_predictions_test each iteration of the loop to do final test set predictions\n",
    "bdtc_predictions_test = bdtc.predict(test_data)\n",
    "    \n",
    "bdtc_weighted_error_rate = np.sum(data_weights * (1 * (bdtc_predictions != train_labels_pm)).astype(\"float\"))\n",
    "\n",
    "# you'll need to save error_rate_alpha each iteration of the loop to do all final predictions\n",
    "error_rate_alpha = np.log((1 - bdtc_weighted_error_rate) / bdtc_weighted_error_rate) / 2\n",
    "    \n",
    "# reweighting: how does this emphasize errors?    \n",
    "data_weights_updated = data_weights * np.exp(-1 * error_rate_alpha * bdtc_predictions * train_labels_pm)\n",
    "data_weights_updated = data_weights_updated / sum(data_weights_updated)\n",
    "data_weights = data_weights_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above algorithm is a *meta algorithm*, meaning we can use any kind of classifier, not just decision trees in the above. Try using another one. Does that change the results?\n",
    "\n",
    "Note that the above is a bit adapted to a classification loss, we'd need to use something like L2 boosting for regression, that is beyond the scope of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
