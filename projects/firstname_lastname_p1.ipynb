{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6UHmLYVhWAN"
   },
   "source": [
    "# Project 1: Digit Classification with KNN and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03M_JSg3hWAO"
   },
   "source": [
    "In this project, you'll implement your own image recognition system for classifying digits. Read through the code and the instructions carefully and add your own code where indicated. Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but <b> please prepare your own write-up (with your own code). </b>\n",
    "\n",
    "If you're interested, check out these links related to digit recognition:\n",
    "\n",
    "* Yann Lecun's MNIST benchmarks: http://yann.lecun.com/exdb/mnist/\n",
    "* Stanford Streetview research and data: http://ufldl.stanford.edu/housenumbers/\n",
    "\n",
    "Finally, if you'd like to get started with Tensorflow, you can read through this tutorial: https://www.tensorflow.org/tutorials/keras/basic_classification. It uses a dataset called \"fashion_mnist\", which is identical in structure to the original digit mnist, but uses images of clothing rather than images of digits. The number of training examples and number of labels is the same. In fact, you can simply replace the code that loads \"fashion_mnist\" with \"mnist\" and everything should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJ9ayCvyhWAP"
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sO1t0ypThWAR"
   },
   "source": [
    "Load the data. Notice that the data gets partitioned into training, development, and test sets. Also, a small subset of the training data called mini_train_data and mini_train_labels gets defined, which you should use in all the experiments below, unless otherwise noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yK9DacchWAS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (70000, 784)\n",
      "label shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the digit data from https://www.openml.org/d/554 or from default local location '~/scikit_learn_data/...'\n",
    "X, Y = fetch_openml(name='mnist_784', return_X_y=True, cache=False)\n",
    "\n",
    "\n",
    "# Rescale grayscale values to [0,1].\n",
    "X = X / 255.0\n",
    "\n",
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "print('data shape: ', X.shape)\n",
    "print('label shape:', Y.shape)\n",
    "\n",
    "# Set some variables to hold test, dev, and training data.\n",
    "test_data, test_labels = X[61000:], Y[61000:]\n",
    "dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
    "train_data, train_labels = X[:60000], Y[:60000]\n",
    "mini_train_data, mini_train_labels = X[:1000], Y[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atc2JpWKhWAV"
   },
   "source": [
    "### Part 1:\n",
    "\n",
    "Show a 10x10 grid that visualizes 10 examples of each digit.\n",
    "\n",
    "Notes:\n",
    "* You can use `plt.rc()` for setting the colormap, for example to black and white.\n",
    "* You can use `plt.subplot()` for creating subplots.\n",
    "* You can use `plt.imshow()` for rendering a matrix.\n",
    "* You can use `np.array.reshape()` for reshaping a 1D feature vector into a 2D matrix (for rendering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "436UeH7JhWAW"
   },
   "outputs": [],
   "source": [
    "#def P1(num_examples=10):\n",
    "\n",
    "### STUDENT START ###\n",
    "    \n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P1(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMQAHr7QhWAX"
   },
   "source": [
    "### Part 2:\n",
    "\n",
    "Produce k-Nearest Neighbors models with k $\\in$ [1,3,5,7,9].  Evaluate and show the accuracy of each model. For the 1-Nearest Neighbor model, additionally show the precision, recall, and F1 for each label. Which digit is the most difficult for the 1-Nearest Neighbor model to recognize?\n",
    "\n",
    "Notes:\n",
    "* Train on the mini train set.\n",
    "* Evaluate performance on the dev set.\n",
    "* You can use `KNeighborsClassifier` to produce a k-nearest neighbor model.\n",
    "* You can use `classification_report` to get precision, recall, and F1 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-it5pn8-hWAY"
   },
   "outputs": [],
   "source": [
    "#def P2(k_values):\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "    \n",
    "### STUDENT END ###\n",
    "\n",
    "#k_values = [1, 3, 5, 7, 9]\n",
    "#P2(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZc9gzn5hWAZ"
   },
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b6YEAzzhWAa"
   },
   "source": [
    "### Part 3:\n",
    "\n",
    "Produce 1-Nearest Neighbor models using training data of various sizes.  Evaluate and show the performance of each model.  Additionally, show the time needed to measure the performance of each model.\n",
    "\n",
    "Notes:\n",
    "* Train on subsets of the train set.  For each subset, take just the first part of the train set without re-ordering.\n",
    "* Evaluate on the dev set.\n",
    "* You can use `KNeighborsClassifier` to produce a k-nearest neighbor model.\n",
    "* You can use `time.time()` to measure elapsed time of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEpNzDEjhWAa"
   },
   "outputs": [],
   "source": [
    "#def P3(train_sizes, accuracies):\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#train_sizes = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600]\n",
    "#accuracies = []\n",
    "#P3(train_sizes, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B56lVsKNhWAc"
   },
   "source": [
    "### Part 4:\n",
    "\n",
    "Produce a linear regression model that predicts accuracy of a 1-Nearest Neighbor model given training set size. Show $R^2$ of the linear regression model.  Show the accuracies predicted for training set sizes 60000, 120000, and 1000000.  Show a lineplot of actual accuracies and predicted accuracies vs. training set size over the range of training set sizes in the training data.  What's wrong with using linear regression here?\n",
    "\n",
    "Apply a transformation to the predictor features and a transformation to the outcome that make the predictions more reasonable.  Show $R^2$ of the improved linear regression model.  Show the accuracies predicted for training set sizes 60000, 120000, and 1000000.  Show a lineplot of actual accuracies and predicted accuracies vs. training set size over the range of training set sizes in the training data - be sure to display accuracies and training set sizes in appropriate units.\n",
    "\n",
    "Notes:\n",
    "* Train the linear regression models on all of the (transformed) accuracies estimated in Problem 3.\n",
    "* Evaluate the linear regression models on all of the (transformed) accuracies estimated in Problem 3.\n",
    "* You can use `LinearRegression` to produce a linear regression model.\n",
    "* Remember that the sklearn `fit()` functions take an input matrix X and output vector Y. So, each input example in X is a vector, even if it contains only a single value.\n",
    "* Hint re: predictor feature transform: Accuracy increases with training set size logarithmically.\n",
    "* Hint re: outcome transform: When y is a number in range 0 to 1, then odds(y)=y/(1-y) is a number in range 0 to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xE_qIJghWAc"
   },
   "outputs": [],
   "source": [
    "#def P4():\n",
    "\n",
    "### STUDENT START ###\n",
    "    \n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYYYL9cGhWAe"
   },
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geAQJjGRhWAe"
   },
   "source": [
    "### Part 5:\n",
    "\n",
    "Produce a 1-Nearest Neighbor model and show the confusion matrix. Which pair of digits does the model confuse most often? Show the images of these most often confused digits.\n",
    "\n",
    "Notes:\n",
    "- Train on the mini train set.\n",
    "- Evaluate performance on the dev set.\n",
    "- You can use `confusion_matrix()` to produce a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bq36xaQohWAf"
   },
   "outputs": [],
   "source": [
    "#def P5():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "    \n",
    "### STUDENT END ###\n",
    "\n",
    "#P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgqMKb-hhWAh"
   },
   "source": [
    "### Part 6:\n",
    "\n",
    "A common image processing technique is to smooth an image by blurring. The idea is that the value of a particular pixel is estimated as the weighted combination of the original value and the values around it. Typically, the blurring is Gaussian, i.e., the weight of a pixel's influence is determined by a Gaussian function over the distance to the relevant pixel.\n",
    "\n",
    "Implement a simplified Gaussian blur filter by just using the 8 neighboring pixels like this: the smoothed value of a pixel is a weighted combination of the original value and the 8 neighboring values.\n",
    "\n",
    "Pick a weight, then produce and evaluate four 1-Nearest Neighbor models by applying your blur filter in these ways:\n",
    "- Do not use the filter\n",
    "- Filter the training data but not the dev data\n",
    "- Filter the dev data but not the training data\n",
    "- Filter both training data and dev data\n",
    "\n",
    "Show the accuracies of the four models evaluated as described.  Try to pick a weight that makes one model's accuracy at least 0.9.\n",
    "\n",
    "Notes:\n",
    "* Train on the (filtered) mini train set.\n",
    "* Evaluate performance on the (filtered) dev set.\n",
    "* There are other Guassian blur filters available, for example in `scipy.ndimage.filters`. You are welcome to experiment with those, but you are likely to get the best results with the simplified version described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSKHmHGshWAi"
   },
   "outputs": [],
   "source": [
    "#def P6():\n",
    "    \n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtgepWfAhWAk"
   },
   "source": [
    "### Part 7:\n",
    "\n",
    "Produce two Naive Bayes models and evaluate their performances.  Recall that Naive Bayes estimates P(feature|label), where each label is a categorical, not a real number.\n",
    "\n",
    "For the first model, map pixel values to either 0 or 1, representing white or black - you should pre-process the data or use `BernoulliNB`'s `binarize` parameter to set the white/black separation threshold to 0.1.  Use `BernoulliNB` to produce the model.\n",
    "\n",
    "For the second model, map pixel values to either 0, 1, or 2, representing white, gray, or black - you should pre-process the data, seting the white/gray/black separation thresholds to 0.1 and 0.9.  Use `MultinomialNB` to produce the model. \n",
    "\n",
    "Show the Bernoulli model accuracy and the Multinomial model accuracy.\n",
    "\n",
    "Notes:\n",
    "* Train on the mini train set.\n",
    "* Evaluate performance on the dev set.\n",
    "* `sklearn`'s Naive Bayes methods can handle real numbers, but for this exercise explicitly do the mapping to categoricals. \n",
    "\n",
    "Does the multinomial version improve the results? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGpH-4IQhWAk"
   },
   "outputs": [],
   "source": [
    "#def P7():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "    \n",
    "### STUDENT END ###\n",
    "\n",
    "#P7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNLrgggohWAm"
   },
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqjbRLg7hWAm"
   },
   "source": [
    "### Part 8:\n",
    "\n",
    "Search across several values of the LaPlace smoothing parameter (alpha) to find its effect on a Bernoulli Naive Bayes model's performance.  Show the accuracy at each alpha value.\n",
    "\n",
    "Notes:\n",
    "* Set binarization threshold to 0.\n",
    "* Train on the mini train set.\n",
    "* Evaluate performance by 5-fold cross-validation. \n",
    "* Use `GridSearchCV(..., ..., cv=..., scoring='accuracy', iid=False)` to vary alpha and evaluate performance by cross-validation.\n",
    "* Cross-validation is based on partitions of the training data, so results will be a bit different than if you had used the dev set to evaluate performance.\n",
    "\n",
    "What is the best value for alpha? What is the accuracy when alpha is near 0? Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AvZ-Wp3hWAn"
   },
   "outputs": [],
   "source": [
    "#def P8(alphas):\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "# alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "# nb = P8(alphas)\n",
    "# print()\n",
    "# print(\"Best alpha = \", nb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yEg9keThWAp"
   },
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B07GDiDdhWAq"
   },
   "source": [
    "### Part 9:\n",
    "\n",
    "Produce a model using Guassian Naive Bayes, which is intended for real-valued features, and evaluate performance. You will notice that it does not work so well. Diagnose the problem and apply a simple fix so that the model accuracy is around the same as for a Bernoulli Naive Bayes model. Show the model accuracy before your fix and the model accuracy after your fix.  Explain your solution.\n",
    "\n",
    "Notes:\n",
    "* Train on the mini train set.\n",
    "* Evaluate performance on the dev set.\n",
    "* Consider the effects of theta and sigma.  These are stored in the model's `theta_` and `sigma_` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBLbTMWChWAq"
   },
   "outputs": [],
   "source": [
    "#def P9():\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SyHTEJohWAt"
   },
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgZMuc1VhWAt"
   },
   "source": [
    "### Part 10:\n",
    "\n",
    "Because Naive Bayes produces a generative model, you can use it to generate digit images.\n",
    "\n",
    "Produce a Bernoulli Naive Bayes model and then use it to generate a 10x20 grid with 20 example images of each digit. Each pixel output should be either 0 or 1, based on comparing some randomly generated number to the estimated probability of the pixel being either 0 or 1.  Show the grid.\n",
    "\n",
    "Notes:\n",
    "* You can use np.random.rand() to generate random numbers from a uniform distribution.\n",
    "* The estimated probability of each pixel being 0 or 1 is stored in the model's `feature_log_prob_` attribute. You can use `np.exp()` to convert a log probability back to a probability.\n",
    "\n",
    "How do the generated digit images compare to the training digit images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktii-Mp-hWAu"
   },
   "outputs": [],
   "source": [
    "#def P10(num_examples):\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P10(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuQd1fTGhWAw"
   },
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksHMg73uhWAx"
   },
   "source": [
    "### Part 11:\n",
    "\n",
    "Recall that a strongly calibrated classifier is rougly 90% accurate when the posterior probability of the predicted class is 0.9. A weakly calibrated classifier is more accurate when the posterior probability of the predicted class is 90% than when it is 80%. A poorly calibrated classifier has no positive correlation between posterior probability and accuracy.  \n",
    "\n",
    "Produce a Bernoulli Naive Bayes model.  Evaluate performance: partition the dev set into several buckets based on the posterior probabilities of the predicted classes - think of a bin in a histogram- and then estimate the accuracy for each bucket. So, for each prediction, find the bucket to which the maximum posterior probability belongs, and update \"correct\" and \"total\" counters accordingly.  Show the accuracy for each bucket.\n",
    "\n",
    "Notes:\n",
    "* Set LaPlace smoothing (alpha) to the optimal value (from part 8).\n",
    "* Set binarization threshold to 0.\n",
    "* Train on the mini train set.\n",
    "* Evaluate perfromance on the dev set.\n",
    "\n",
    "How would you characterize the calibration for this Bernoulli Naive Bayes model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1N-St12hWAy"
   },
   "outputs": [],
   "source": [
    "#def P11(buckets, correct, total):\n",
    "    \n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "                \n",
    "### STUDENT END ###\n",
    "\n",
    "# buckets = [0.5, 0.9, 0.999, 0.99999, 0.9999999, 0.999999999, 0.99999999999, 0.9999999999999, 1.0]\n",
    "# correct = [0 for i in buckets]\n",
    "# total = [0 for i in buckets]\n",
    "\n",
    "# P11(buckets, correct, total)\n",
    "\n",
    "# for i in range(len(buckets)):\n",
    "#     accuracy = 0.0\n",
    "#     if (total[i] > 0): accuracy = correct[i] / total[i]\n",
    "#     print('p(pred) is %.13f to %.13f    total = %3d    accuracy = %.3f' % (0 if i==0 else buckets[i-1], buckets[i], total[i], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-4qQsrrhWA1"
   },
   "source": [
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLDISyh4hWA1"
   },
   "source": [
    "### Part 12 EXTRA CREDIT:\n",
    "\n",
    "Design new features to see if you can produce a Bernoulli Naive Bayes model with better performance.  Show the accuracy of a model based on the original features and the accuracy of the model based on the new features.\n",
    "\n",
    "Here are a few ideas to get you started:\n",
    "- Try summing or averaging the pixel values in each row.\n",
    "- Try summing or averaging the pixel values in each column.\n",
    "- Try summing or averaging the pixel values in each square block. (pick various block sizes)\n",
    "- Try counting the number of enclosed regions. (8 usually has 2 enclosed regions, 9 usually has 1, and 7 usually has 0)\n",
    "\n",
    "Notes:\n",
    "* Train on the mini train set (enhanced to comprise the new features).\n",
    "* Evaulate performance on the dev set.\n",
    "* Ensure that your code is well commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-P7h-t2ThWA2"
   },
   "outputs": [],
   "source": [
    "#def P12():\n",
    "\n",
    "### STUDENT START ###\n",
    "\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P12()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "firstname_lastname_p1.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/MIDS-W207/Master/blob/master/Projects/firstname_lastname_p1.ipynb",
     "timestamp": 1557957807607
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
